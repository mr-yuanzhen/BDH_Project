{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LEC_yWnK8har"
   },
   "source": [
    "# **FarSight: Long-Term Disease Prediction Using Unstructured Clinical Nursing Notes**\n",
    "by Tushaar Gangavarapu, Gokul S Krishnan, Sowmya Kamath S, Jayakumar Jeganathan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lvE5I3mx9PBH"
   },
   "source": [
    "In this paper, the authors use nursing notes to predict the onset disease ICD-9 code. This paper adopted a multi-class, multi-label approach. Here are the general process:\n",
    "\n",
    "1.   Data: MIMIC-III (v1.4), total 4 tables was used (noteevents, admissions, patients and diagnosis_icd)\n",
    "2.   Data Preprocess:\n",
    "*   The MIMIC-III (v1.4) database contains 223,556 nursing notes among 2,083,180 note events, corresponding to 7,704 distinct patients. (filtered by CATEGORY = 'Nursing')\n",
    "*   Using the age at the time of admission to the ICU, the records of neonates(age below 15) were identified and discarded.\n",
    "*   Only the first admission to the ICU for each MIMIC-III patient was considered, and all the later admissions were discarded. For each admission, there are multiple ICD-9 diagnosis and nursing notes\n",
    "*   The nursing notes are further preprocessed by tokenization, removing stop words, removing duplicating notes, etc.  \n",
    "*   The grouped ICD-9 code is used as the classification labels\n",
    "*   For the naive data, all the nursing notes of a patient are aggregated before merging it with all classification labels\n",
    "*   For the FarSight data, each nursing note is merged with all classification labels\n",
    "3.  Vectorization:\n",
    "The following vectorization method is used for dimension reduction\n",
    "*   NMF on BoW with Semantic Coherence(SC)\n",
    "\n",
    "4.   Predictive Modeling:\n",
    "Various methods are used to achieve a higher prediction accuracy.\n",
    "*   Multi-Layer Perceptron(MLP)\n",
    "*   Pretrained Bio_ClinicalBERT\n",
    "*   Finetuned Bio_ClinicalBERT\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "Cz73vScn3rrT",
    "outputId": "a241818b-3a13-4886-f3d3-90cf4c6c5de0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Downloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/62.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting gensim\n",
      "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.15.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow\n",
      "  Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting numpy\n",
      "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting scipy\n",
      "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting smart-open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Downloading absl_py-2.2.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting packaging (from tensorflow)\n",
      "  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow)\n",
      "  Downloading protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Collecting requests<3,>=2.21.0 (from tensorflow)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting setuptools (from tensorflow)\n",
      "  Downloading setuptools-80.7.1-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting six>=1.12.0 (from tensorflow)\n",
      "  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting typing-extensions>=3.6.6 (from tensorflow)\n",
      "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Downloading wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.71.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
      "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow)\n",
      "  Downloading keras-3.9.2-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting h5py>=3.11.0 (from tensorflow)\n",
      "  Downloading h5py-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
      "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting rich (from keras>=3.5.0->tensorflow)\n",
      "  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow)\n",
      "  Downloading namex-0.0.9-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow)\n",
      "  Downloading optree-0.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting charset-normalizer<4,>=2 (from requests<3,>=2.21.0->tensorflow)\n",
      "  Downloading charset_normalizer-3.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2.21.0->tensorflow)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.21.0->tensorflow)\n",
      "  Downloading urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorflow)\n",
      "  Downloading certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Downloading markdown-3.8-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow)\n",
      "  Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.5.0->tensorflow)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting pygments<3.0.0,>=2.13.0 (from rich->keras>=3.5.0->tensorflow)\n",
      "  Downloading pygments-2.19.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m95.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m100.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.9/644.9 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.2.2-py3-none-any.whl (135 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.71.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m121.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.9.2-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m93.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m128.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Downloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m131.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading setuptools-80.7.1-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m79.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m104.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.2/83.2 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading packaging-25.0-py3-none-any.whl (66 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.6/159.6 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading charset_normalizer-3.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (147 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.3/147.3 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown-3.8-py3-none-any.whl (106 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.2/106.2 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m138.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.7/128.7 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading namex-0.0.9-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (410 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.4/410.4 kB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.2/243.2 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
      "Downloading pygments-2.19.1-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, wheel, urllib3, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, six, setuptools, pygments, protobuf, packaging, opt-einsum, numpy, mdurl, MarkupSafe, markdown, idna, grpcio, gast, charset-normalizer, certifi, absl-py, werkzeug, smart-open, scipy, requests, optree, ml-dtypes, markdown-it-py, h5py, google-pasta, astunparse, tensorboard, rich, gensim, keras, tensorflow\n",
      "  Attempting uninstall: namex\n",
      "    Found existing installation: namex 0.0.9\n",
      "    Uninstalling namex-0.0.9:\n",
      "      Successfully uninstalled namex-0.0.9\n",
      "  Attempting uninstall: libclang\n",
      "    Found existing installation: libclang 18.1.1\n",
      "    Uninstalling libclang-18.1.1:\n",
      "      Successfully uninstalled libclang-18.1.1\n",
      "  Attempting uninstall: flatbuffers\n",
      "    Found existing installation: flatbuffers 25.2.10\n",
      "    Uninstalling flatbuffers-25.2.10:\n",
      "      Successfully uninstalled flatbuffers-25.2.10\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.17.2\n",
      "    Uninstalling wrapt-1.17.2:\n",
      "      Successfully uninstalled wrapt-1.17.2\n",
      "  Attempting uninstall: wheel\n",
      "    Found existing installation: wheel 0.45.1\n",
      "    Uninstalling wheel-0.45.1:\n",
      "      Successfully uninstalled wheel-0.45.1\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.4.0\n",
      "    Uninstalling urllib3-2.4.0:\n",
      "      Successfully uninstalled urllib3-2.4.0\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.13.2\n",
      "    Uninstalling typing_extensions-4.13.2:\n",
      "      Successfully uninstalled typing_extensions-4.13.2\n",
      "  Attempting uninstall: termcolor\n",
      "    Found existing installation: termcolor 3.1.0\n",
      "    Uninstalling termcolor-3.1.0:\n",
      "      Successfully uninstalled termcolor-3.1.0\n",
      "  Attempting uninstall: tensorflow-io-gcs-filesystem\n",
      "    Found existing installation: tensorflow-io-gcs-filesystem 0.37.1\n",
      "    Uninstalling tensorflow-io-gcs-filesystem-0.37.1:\n",
      "      Successfully uninstalled tensorflow-io-gcs-filesystem-0.37.1\n",
      "  Attempting uninstall: tensorboard-data-server\n",
      "    Found existing installation: tensorboard-data-server 0.7.2\n",
      "    Uninstalling tensorboard-data-server-0.7.2:\n",
      "      Successfully uninstalled tensorboard-data-server-0.7.2\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.17.0\n",
      "    Uninstalling six-1.17.0:\n",
      "      Successfully uninstalled six-1.17.0\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 75.2.0\n",
      "    Uninstalling setuptools-75.2.0:\n",
      "      Successfully uninstalled setuptools-75.2.0\n",
      "  Attempting uninstall: pygments\n",
      "    Found existing installation: Pygments 2.19.1\n",
      "    Uninstalling Pygments-2.19.1:\n",
      "      Successfully uninstalled Pygments-2.19.1\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.29.4\n",
      "    Uninstalling protobuf-5.29.4:\n",
      "      Successfully uninstalled protobuf-5.29.4\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 24.2\n",
      "    Uninstalling packaging-24.2:\n",
      "      Successfully uninstalled packaging-24.2\n",
      "  Attempting uninstall: opt-einsum\n",
      "    Found existing installation: opt_einsum 3.4.0\n",
      "    Uninstalling opt_einsum-3.4.0:\n",
      "      Successfully uninstalled opt_einsum-3.4.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "  Attempting uninstall: mdurl\n",
      "    Found existing installation: mdurl 0.1.2\n",
      "    Uninstalling mdurl-0.1.2:\n",
      "      Successfully uninstalled mdurl-0.1.2\n",
      "  Attempting uninstall: MarkupSafe\n",
      "    Found existing installation: MarkupSafe 3.0.2\n",
      "    Uninstalling MarkupSafe-3.0.2:\n",
      "      Successfully uninstalled MarkupSafe-3.0.2\n",
      "  Attempting uninstall: markdown\n",
      "    Found existing installation: Markdown 3.8\n",
      "    Uninstalling Markdown-3.8:\n",
      "      Successfully uninstalled Markdown-3.8\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.10\n",
      "    Uninstalling idna-3.10:\n",
      "      Successfully uninstalled idna-3.10\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.71.0\n",
      "    Uninstalling grpcio-1.71.0:\n",
      "      Successfully uninstalled grpcio-1.71.0\n",
      "  Attempting uninstall: gast\n",
      "    Found existing installation: gast 0.6.0\n",
      "    Uninstalling gast-0.6.0:\n",
      "      Successfully uninstalled gast-0.6.0\n",
      "  Attempting uninstall: charset-normalizer\n",
      "    Found existing installation: charset-normalizer 3.4.2\n",
      "    Uninstalling charset-normalizer-3.4.2:\n",
      "      Successfully uninstalled charset-normalizer-3.4.2\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2025.4.26\n",
      "    Uninstalling certifi-2025.4.26:\n",
      "      Successfully uninstalled certifi-2025.4.26\n",
      "  Attempting uninstall: absl-py\n",
      "    Found existing installation: absl-py 1.4.0\n",
      "    Uninstalling absl-py-1.4.0:\n",
      "      Successfully uninstalled absl-py-1.4.0\n",
      "  Attempting uninstall: werkzeug\n",
      "    Found existing installation: Werkzeug 3.1.3\n",
      "    Uninstalling Werkzeug-3.1.3:\n",
      "      Successfully uninstalled Werkzeug-3.1.3\n",
      "  Attempting uninstall: smart-open\n",
      "    Found existing installation: smart-open 7.1.0\n",
      "    Uninstalling smart-open-7.1.0:\n",
      "      Successfully uninstalled smart-open-7.1.0\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.15.3\n",
      "    Uninstalling scipy-1.15.3:\n",
      "      Successfully uninstalled scipy-1.15.3\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.32.3\n",
      "    Uninstalling requests-2.32.3:\n",
      "      Successfully uninstalled requests-2.32.3\n",
      "  Attempting uninstall: optree\n",
      "    Found existing installation: optree 0.15.0\n",
      "    Uninstalling optree-0.15.0:\n",
      "      Successfully uninstalled optree-0.15.0\n",
      "  Attempting uninstall: ml-dtypes\n",
      "    Found existing installation: ml-dtypes 0.4.1\n",
      "    Uninstalling ml-dtypes-0.4.1:\n",
      "      Successfully uninstalled ml-dtypes-0.4.1\n",
      "  Attempting uninstall: markdown-it-py\n",
      "    Found existing installation: markdown-it-py 3.0.0\n",
      "    Uninstalling markdown-it-py-3.0.0:\n",
      "      Successfully uninstalled markdown-it-py-3.0.0\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.13.0\n",
      "    Uninstalling h5py-3.13.0:\n",
      "      Successfully uninstalled h5py-3.13.0\n",
      "  Attempting uninstall: google-pasta\n",
      "    Found existing installation: google-pasta 0.2.0\n",
      "    Uninstalling google-pasta-0.2.0:\n",
      "      Successfully uninstalled google-pasta-0.2.0\n",
      "  Attempting uninstall: astunparse\n",
      "    Found existing installation: astunparse 1.6.3\n",
      "    Uninstalling astunparse-1.6.3:\n",
      "      Successfully uninstalled astunparse-1.6.3\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.18.0\n",
      "    Uninstalling tensorboard-2.18.0:\n",
      "      Successfully uninstalled tensorboard-2.18.0\n",
      "  Attempting uninstall: rich\n",
      "    Found existing installation: rich 13.9.4\n",
      "    Uninstalling rich-13.9.4:\n",
      "      Successfully uninstalled rich-13.9.4\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 3.8.0\n",
      "    Uninstalling keras-3.8.0:\n",
      "      Successfully uninstalled keras-3.8.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.18.0\n",
      "    Uninstalling tensorflow-2.18.0:\n",
      "      Successfully uninstalled tensorflow-2.18.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
      "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
      "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.19.0 which is incompatible.\n",
      "langchain-core 0.3.59 requires packaging<25,>=23.2, but you have packaging 25.0 which is incompatible.\n",
      "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.19.0 which is incompatible.\n",
      "bigframes 2.4.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
      "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.19.0 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed MarkupSafe-3.0.2 absl-py-2.2.2 astunparse-1.6.3 certifi-2025.4.26 charset-normalizer-3.4.2 flatbuffers-25.2.10 gast-0.6.0 gensim-4.3.3 google-pasta-0.2.0 grpcio-1.71.0 h5py-3.13.0 idna-3.10 keras-3.9.2 libclang-18.1.1 markdown-3.8 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.5.1 namex-0.0.9 numpy-1.26.4 opt-einsum-3.4.0 optree-0.15.0 packaging-25.0 protobuf-5.29.4 pygments-2.19.1 requests-2.32.3 rich-14.0.0 scipy-1.13.1 setuptools-80.7.1 six-1.17.0 smart-open-7.1.0 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 tensorflow-io-gcs-filesystem-0.37.1 termcolor-3.1.0 typing-extensions-4.13.2 urllib3-2.4.0 werkzeug-3.1.3 wheel-0.45.1 wrapt-1.17.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "9699389ba6de4d27ac4408557dfe61f8",
       "pip_warning": {
        "packages": [
         "_distutils_hack",
         "certifi",
         "pkg_resources",
         "six"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install --upgrade --force-reinstall numpy gensim scipy tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "90tWG7fFahjs",
    "outputId": "432b4029-df1f-425b-eeec-1143a3f1cc7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scispacy\n",
      "  Downloading scispacy-0.5.5-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting spacy<3.8.0,>=3.7.0 (from scispacy)\n",
      "  Downloading spacy-3.7.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from scispacy) (1.13.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scispacy) (2.32.3)\n",
      "Collecting conllu (from scispacy)\n",
      "  Downloading conllu-6.0.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from scispacy) (1.26.4)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from scispacy) (1.5.0)\n",
      "Requirement already satisfied: scikit-learn>=0.20.3 in /usr/local/lib/python3.11/dist-packages (from scispacy) (1.6.1)\n",
      "Collecting pysbd (from scispacy)\n",
      "  Downloading pysbd-0.3.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting nmslib-metabrainz==2.1.3 (from scispacy)\n",
      "  Downloading nmslib_metabrainz-2.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (956 bytes)\n",
      "Collecting pybind11>=2.2.3 (from nmslib-metabrainz==2.1.3->scispacy)\n",
      "  Downloading pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from nmslib-metabrainz==2.1.3->scispacy) (5.9.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->scispacy) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->scispacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->scispacy) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->scispacy) (2025.4.26)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.3->scispacy) (3.6.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (3.0.9)\n",
      "Collecting thinc<8.3.0,>=8.2.2 (from spacy<3.8.0,>=3.7.0->scispacy)\n",
      "  Downloading thinc-8.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (0.15.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (4.67.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (2.11.4)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (80.7.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (25.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->scispacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->scispacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->scispacy) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->scispacy) (4.13.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->scispacy) (0.4.0)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->scispacy)\n",
      "  Downloading blis-0.7.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->scispacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->scispacy) (8.2.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->scispacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->scispacy) (14.0.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->scispacy) (0.21.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->scispacy) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->scispacy) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->scispacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->scispacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->scispacy) (2.19.1)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->scispacy) (1.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->scispacy) (0.1.2)\n",
      "Downloading scispacy-0.5.5-py3-none-any.whl (46 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nmslib_metabrainz-2.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading spacy-3.7.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m93.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading conllu-6.0.0-py3-none-any.whl (16 kB)\n",
      "Downloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.3/243.3 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading thinc-8.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (920 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m920.2/920.2 kB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading blis-0.7.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m117.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pysbd, pybind11, conllu, blis, nmslib-metabrainz, thinc, spacy, scispacy\n",
      "  Attempting uninstall: blis\n",
      "    Found existing installation: blis 1.3.0\n",
      "    Uninstalling blis-1.3.0:\n",
      "      Successfully uninstalled blis-1.3.0\n",
      "  Attempting uninstall: thinc\n",
      "    Found existing installation: thinc 8.3.6\n",
      "    Uninstalling thinc-8.3.6:\n",
      "      Successfully uninstalled thinc-8.3.6\n",
      "  Attempting uninstall: spacy\n",
      "    Found existing installation: spacy 3.8.5\n",
      "    Uninstalling spacy-3.8.5:\n",
      "      Successfully uninstalled spacy-3.8.5\n",
      "Successfully installed blis-0.7.11 conllu-6.0.0 nmslib-metabrainz-2.1.3 pybind11-2.13.6 pysbd-0.3.4 scispacy-0.5.5 spacy-3.7.5 thinc-8.2.5\n"
     ]
    }
   ],
   "source": [
    "!pip install scispacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "2aGrQhBIamIh",
    "outputId": "0b923c17-b67d-433d-a7d8-b7d76f6c5f5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_sm-0.5.1.tar.gz\n",
      "  Downloading https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_sm-0.5.1.tar.gz (15.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting spacy<3.5.0,>=3.4.1 (from en_core_sci_sm==0.5.1)\n",
      "  Downloading spacy-3.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (24 kB)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_sm==0.5.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_sm==0.5.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_sm==0.5.1) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_sm==0.5.1) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_sm==0.5.1) (3.0.9)\n",
      "Collecting thinc<8.2.0,>=8.1.0 (from spacy<3.5.0,>=3.4.1->en_core_sci_sm==0.5.1)\n",
      "  Downloading thinc-8.1.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.1.0,>=0.9.1 (from spacy<3.5.0,>=3.4.1->en_core_sci_sm==0.5.1)\n",
      "  Downloading wasabi-0.10.1-py3-none-any.whl.metadata (28 kB)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_sm==0.5.1) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_sm==0.5.1) (2.0.10)\n",
      "Collecting typer<0.8.0,>=0.3.0 (from spacy<3.5.0,>=3.4.1->en_core_sci_sm==0.5.1)\n",
      "  Downloading typer-0.7.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting pathy>=0.3.5 (from spacy<3.5.0,>=3.4.1->en_core_sci_sm==0.5.1)\n",
      "  Downloading pathy-0.11.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting smart-open<7.0.0,>=5.2.1 (from spacy<3.5.0,>=3.4.1->en_core_sci_sm==0.5.1)\n",
      "  Downloading smart_open-6.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_sm==0.5.1) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_sm==0.5.1) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_sm==0.5.1) (2.32.3)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 (from spacy<3.5.0,>=3.4.1->en_core_sci_sm==0.5.1)\n",
      "  Downloading pydantic-1.10.22-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (154 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.1/154.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_sm==0.5.1) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_sm==0.5.1) (80.7.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_sm==0.5.1) (25.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_core_sci_sm==0.5.1) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.5.0,>=3.4.1->en_core_sci_sm==0.5.1) (1.3.0)\n",
      "Collecting pathlib-abc==0.1.1 (from pathy>=0.3.5->spacy<3.5.0,>=3.4.1->en_core_sci_sm==0.5.1)\n",
      "  Downloading pathlib_abc-0.1.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.1->en_core_sci_sm==0.5.1) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->en_core_sci_sm==0.5.1) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->en_core_sci_sm==0.5.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->en_core_sci_sm==0.5.1) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->en_core_sci_sm==0.5.1) (2025.4.26)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.1->en_core_sci_sm==0.5.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.1->en_core_sci_sm==0.5.1) (0.1.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.11/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.1->en_core_sci_sm==0.5.1) (8.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.5.0,>=3.4.1->en_core_sci_sm==0.5.1) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.5.0,>=3.4.1->en_core_sci_sm==0.5.1) (1.2.1)\n",
      "Downloading spacy-3.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pathy-0.11.0-py3-none-any.whl (47 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pathlib_abc-0.1.1-py3-none-any.whl (23 kB)\n",
      "Downloading pydantic-1.10.22-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m108.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading thinc-8.1.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (917 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m917.4/917.4 kB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typer-0.7.0-py3-none-any.whl (38 kB)\n",
      "Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
      "Building wheels for collected packages: en_core_sci_sm\n",
      "  Building wheel for en_core_sci_sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for en_core_sci_sm: filename=en_core_sci_sm-0.5.1-py3-none-any.whl size=15870889 sha256=216d0237ec124f5cf8d63d1c255dfd518050bb1b7829442fb76e6acbad790e2c\n",
      "  Stored in directory: /root/.cache/pip/wheels/f0/4d/eb/0d4f64bca5fb19915b27acb2aaab5391404b0f76092d41d96d\n",
      "Successfully built en_core_sci_sm\n",
      "Installing collected packages: wasabi, typer, smart-open, pydantic, pathlib-abc, pathy, thinc, spacy, en_core_sci_sm\n",
      "  Attempting uninstall: wasabi\n",
      "    Found existing installation: wasabi 1.1.3\n",
      "    Uninstalling wasabi-1.1.3:\n",
      "      Successfully uninstalled wasabi-1.1.3\n",
      "  Attempting uninstall: typer\n",
      "    Found existing installation: typer 0.15.3\n",
      "    Uninstalling typer-0.15.3:\n",
      "      Successfully uninstalled typer-0.15.3\n",
      "  Attempting uninstall: smart-open\n",
      "    Found existing installation: smart-open 7.1.0\n",
      "    Uninstalling smart-open-7.1.0:\n",
      "      Successfully uninstalled smart-open-7.1.0\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.11.4\n",
      "    Uninstalling pydantic-2.11.4:\n",
      "      Successfully uninstalled pydantic-2.11.4\n",
      "  Attempting uninstall: thinc\n",
      "    Found existing installation: thinc 8.2.5\n",
      "    Uninstalling thinc-8.2.5:\n",
      "      Successfully uninstalled thinc-8.2.5\n",
      "  Attempting uninstall: spacy\n",
      "    Found existing installation: spacy 3.7.5\n",
      "    Uninstalling spacy-3.7.5:\n",
      "      Successfully uninstalled spacy-3.7.5\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "scispacy 0.5.5 requires spacy<3.8.0,>=3.7.0, but you have spacy 3.4.4 which is incompatible.\n",
      "langchain-core 0.3.59 requires packaging<25,>=23.2, but you have packaging 25.0 which is incompatible.\n",
      "langchain-core 0.3.59 requires pydantic<3.0.0,>=2.5.2; python_full_version < \"3.12.4\", but you have pydantic 1.10.22 which is incompatible.\n",
      "langchain 0.3.25 requires pydantic<3.0.0,>=2.7.4, but you have pydantic 1.10.22 which is incompatible.\n",
      "albumentations 2.0.6 requires pydantic>=2.9.2, but you have pydantic 1.10.22 which is incompatible.\n",
      "google-genai 1.15.0 requires pydantic<3.0.0,>=2.0.0, but you have pydantic 1.10.22 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed en_core_sci_sm-0.5.1 pathlib-abc-0.1.1 pathy-0.11.0 pydantic-1.10.22 smart-open-6.4.0 spacy-3.4.4 thinc-8.1.12 typer-0.7.0 wasabi-0.10.1\n"
     ]
    }
   ],
   "source": [
    "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_sm-0.5.1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9I-aa1N7P-WW"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "from google.colab.data_table import DataTable\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Conv1D, Conv2D, Reshape, TimeDistributed, LSTM\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import f1_score, matthews_corrcoef, roc_auc_score, average_precision_score\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.nmf import Nmf\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "DataTable.max_columns = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "0Fumm1t3UsOX",
    "outputId": "01cd6825-9422-4292-fec4-8d3a89a2dbbc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c7ZOJkaU_55g"
   },
   "source": [
    "## **1. Set up data directory and loading data**\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MhK7802BOpPr",
    "outputId": "f4b44ec9-c398-495e-de5d-d32f94df2f58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: /content\n",
      "Mounted at /content/gdrive\n",
      "Working Directory: /content/gdrive/My Drive/BDH_Project/data\n"
     ]
    }
   ],
   "source": [
    "# View and modify the working path\n",
    "import os\n",
    "from google.colab import drive\n",
    "\n",
    "# View current working directory\n",
    "print(\"Current Working Directory:\", os.getcwd())\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "# Change working directory to your file position\n",
    "path = \"/content/gdrive/My Drive/BDH_Project/data\"\n",
    "os.chdir(path)\n",
    "\n",
    "# Confirm the change\n",
    "print(\"Working Directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "udeLPwmvN20z",
    "outputId": "45078be3-6a15-4eb9-caf4-66ff2ace3506"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-6923350344a1>:3: DtypeWarning: Columns (4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_note = pd.read_csv('NOTEEVENTS.csv')\n"
     ]
    }
   ],
   "source": [
    "# import data\n",
    "df_icd9 = pd.read_csv('DIAGNOSES_ICD.csv.gz', compression='gzip')\n",
    "df_note = pd.read_csv('NOTEEVENTS.csv')\n",
    "df_ad = pd.read_csv('ADMISSIONS.csv')\n",
    "df_pat = pd.read_csv('PATIENTS.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CFek6TiRDiQ-"
   },
   "source": [
    "## **2. Data Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THfYH6c9D1PI"
   },
   "source": [
    "## **2.1. Statistic of the Nursing Notes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D3OT3fo-DuC8",
    "outputId": "8caef994-867e-4871-f466-ce65f03f9e0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan in Notes dataset:ROW_ID               0\n",
      "SUBJECT_ID           0\n",
      "HADM_ID         231836\n",
      "CHARTDATE            0\n",
      "CHARTTIME       316566\n",
      "STORETIME       836776\n",
      "CATEGORY             0\n",
      "DESCRIPTION          0\n",
      "CGID            836776\n",
      "ISERROR        2082294\n",
      "TEXT                 0\n",
      "dtype: int64\n",
      "\n",
      "Total Row in Notes dataset:2083180\n",
      "\n",
      "CATEGORY\n",
      "Nursing/other        822497\n",
      "Radiology            522279\n",
      "Nursing              223556\n",
      "ECG                  209051\n",
      "Physician            141624\n",
      "Discharge summary     59652\n",
      "Echo                  45794\n",
      "Respiratory           31739\n",
      "Nutrition              9418\n",
      "General                8301\n",
      "Rehab Services         5431\n",
      "Social Work            2670\n",
      "Case Management         967\n",
      "Pharmacy                103\n",
      "Consult                  98\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f\"Nan in Notes dataset:{df_note.isna().sum()}\")\n",
    "print()\n",
    "print(f\"Total Row in Notes dataset:{df_note.shape[0]}\")\n",
    "print()\n",
    "print(df_note.CATEGORY.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XO6Cc7JbFB9B"
   },
   "source": [
    "From TABLE 1 in the paper, The MIMIC-III (v1.4) database contains 223,556 nursing notes among 2,083,180 note events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zFOovRHRGEGz"
   },
   "source": [
    "### **2.2 Filter out Nursing only data using CATEGORY == Nursing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RoP_Dt3qF6EB",
    "outputId": "70ca7bfe-c87b-4d64-ce6c-cde21d92d484"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total notes in Nursing dataset:223556\n",
      "Unique patients in Nursing dataset:7704\n"
     ]
    }
   ],
   "source": [
    "df_nursing = df_note[df_note['CATEGORY'] == 'Nursing']\n",
    "print(f\"Total notes in Nursing dataset:{df_nursing.shape[0]}\")\n",
    "print(f\"Unique patients in Nursing dataset:{df_nursing['SUBJECT_ID'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Ro_vSe4PUpL",
    "outputId": "383c86c0-3173-4bce-f373-c4b0a864a015"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing notes: 100%|██████████| 223556/223556 [09:19<00:00, 399.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 Parameter     Total\n",
      "0             Total clinical nursing notes    223556\n",
      "1     Total sentences in the nursing notes   4694552\n",
      "2         Total words in the nursing notes  77852407\n",
      "3  Total unique words in the nursing notes         0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming your nursing notes are in a DataFrame\n",
    "# df_nursing = pd.read_csv(...) or however you load your notes\n",
    "# Make sure 'TEXT' column exists\n",
    "notes = df_nursing['TEXT'].dropna().astype(str)\n",
    "\n",
    "total_notes = len(notes)\n",
    "total_sentences = 0\n",
    "total_words = 0\n",
    "unique_words = set()\n",
    "\n",
    "for note in tqdm(notes, desc=\"Processing notes\"):\n",
    "    sentences = sent_tokenize(note)\n",
    "    words = word_tokenize(note)\n",
    "\n",
    "    total_sentences += len(sentences)\n",
    "    total_words += len(words)\n",
    "\n",
    "\n",
    "# Final stats\n",
    "stats = {\n",
    "    \"Total clinical nursing notes\": total_notes,\n",
    "    \"Total sentences in the nursing notes\": total_sentences,\n",
    "    \"Total words in the nursing notes\": total_words,\n",
    "    \"Total unique words in the nursing notes\": len(unique_words)\n",
    "}\n",
    "\n",
    "# Display the results as a DataFrame (you can print or save to CSV/LaTeX)\n",
    "df_stats = pd.DataFrame.from_dict(stats, orient='index', columns=['Total'])\n",
    "df_stats.reset_index(inplace=True)\n",
    "df_stats.columns = ['Parameter', 'Total']\n",
    "print(df_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZ3frVdNpjzR"
   },
   "source": [
    "### **2.3 Filter out Admission table to only keep first admission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XtcrQl2Sn1na",
    "outputId": "6512bb4b-cc9b-4d33-c44b-a3ac4473c7a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique patients in Admission before filter:46520\n",
      "Data shape in Admission before filter:(58976, 19)\n",
      "\n",
      "Unique patients in Admission after filter:46520\n",
      "Data shape in Admission after filter:(46520, 19)\n"
     ]
    }
   ],
   "source": [
    "df_ad['ADMITTIME'] = pd.to_datetime(df_ad['ADMITTIME'])\n",
    "df_ad_sorted = df_ad.sort_values(['SUBJECT_ID', 'ADMITTIME'])\n",
    "df_ad_final = df_ad_sorted.drop_duplicates(subset=['SUBJECT_ID'], keep='first')\n",
    "print(f\"Unique patients in Admission before filter:{df_ad['SUBJECT_ID'].nunique()}\")\n",
    "print(f\"Data shape in Admission before filter:{df_ad.shape}\")\n",
    "print()\n",
    "print(f\"Unique patients in Admission after filter:{df_ad_final['SUBJECT_ID'].nunique()}\")\n",
    "print(f\"Data shape in Admission after filter:{df_ad_final.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "24NbQFXPn_Pp",
    "outputId": "ba4ae0be-faa3-4692-c2cb-4d5bd2125cc3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot no. admission per subject\n",
    "# Filter admissions to only include subjects from nursing notes\n",
    "subjects_in_nursing = df_nursing['SUBJECT_ID'].unique()\n",
    "df_ad_filtered = df_ad[df_ad['SUBJECT_ID'].isin(subjects_in_nursing)]\n",
    "admission_counts = df_ad_filtered['SUBJECT_ID'].value_counts()\n",
    "distribution = admission_counts.value_counts().sort_index()\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(distribution.index, distribution.values)\n",
    "plt.xlabel('Number of Admissions')\n",
    "plt.ylabel('Number of Patients')\n",
    "plt.title('Distribution of Number of Admissions per Patient')\n",
    "plt.xticks(distribution.index)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8DL0B8Hi1gQn"
   },
   "source": [
    "### **2.4 Join admission table and patient table, filter out patient with age > 15**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BoVyhrD5T3xN",
    "outputId": "88a39362-3423-47db-8924-73468fcce926"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique patients after filter:38645\n",
      "Data shape after filter:(38645, 27)\n",
      "Median age:65.60232717316906\n"
     ]
    }
   ],
   "source": [
    "# Merge with patients table\n",
    "merged_df = pd.merge(df_ad_final, df_pat, on='SUBJECT_ID')\n",
    "\n",
    "# Calculate age at admission\n",
    "merged_df['DOB'] = pd.to_datetime(merged_df['DOB'])\n",
    "#merged_df['AGE'] = merged_df['ADMITTIME'].dt.year- merged_df['DOB'].dt.year\n",
    "def calculate_precise_age(row):\n",
    "    dob = row['DOB']\n",
    "    admit = row['ADMITTIME']\n",
    "    # Base age in full years\n",
    "    age_years = admit.year - dob.year\n",
    "\n",
    "    # Try to build the \"birthday in admit year\"\n",
    "    try:\n",
    "        birthday_this_year = pd.Timestamp(admit.year, dob.month, dob.day)\n",
    "    except ValueError:\n",
    "        # Handle Feb 29 in non-leap years\n",
    "        birthday_this_year = pd.Timestamp(admit.year, dob.month, 28)\n",
    "\n",
    "    # Subtract 1 year if they haven't had their birthday yet\n",
    "    if admit < birthday_this_year:\n",
    "        age_years -= 1\n",
    "        birthday_this_year = birthday_this_year - DateOffset(years=1)\n",
    "\n",
    "    # Add fractional year based on days since birthday\n",
    "    partial_year = (admit - birthday_this_year).days / 365.25\n",
    "    return age_years + partial_year\n",
    "\n",
    "# Apply the function row by row\n",
    "merged_df['AGE'] = merged_df.apply(calculate_precise_age, axis=1)\n",
    "\n",
    "# Filter out age <= 15\n",
    "filtered_df = merged_df[merged_df['AGE'] > 15]\n",
    "print(f\"Unique patients after filter:{filtered_df['SUBJECT_ID'].nunique()}\")\n",
    "print(f\"Data shape after filter:{filtered_df.shape}\")\n",
    "print(f\"Median age:{filtered_df['AGE'].median()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySdCCz_srz3X"
   },
   "source": [
    "### **2.5 Join merged admission_patient table with note table, remove error and duplicate**\n",
    "#### Target number is 6532 patients and 140792 clinical notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LBGfurG-vFll",
    "outputId": "b282daf3-ae33-4096-be02-87158fab6235"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique number of subjects:6521\n",
      "Total number of notes:156654\n"
     ]
    }
   ],
   "source": [
    "merged_with_notes = pd.merge(filtered_df, df_nursing, on=[\"SUBJECT_ID\", \"HADM_ID\"], how=\"inner\")\n",
    "print(f\"Unique number of subjects:{merged_with_notes['SUBJECT_ID'].nunique()}\")\n",
    "print(f\"Total number of notes:{merged_with_notes.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U17gPXIevIgG"
   },
   "outputs": [],
   "source": [
    "# sort the merged table to make sure it in chronically order\n",
    "merged_with_notes['CHARTTIME'] = pd.to_datetime(merged_with_notes['CHARTTIME'])\n",
    "merged_with_notes = merged_with_notes.sort_values(['SUBJECT_ID', 'CHARTTIME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w5m3RI2OvMMw",
    "outputId": "8997f99e-15ed-4299-83c6-dfaaff9cb067"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique number of subjects:6520\n",
      "Unique number of HADM:6520\n",
      "Total number of notes:140681\n"
     ]
    }
   ],
   "source": [
    "#target number is 6532 patients and 140792 clinical notes\n",
    "filter_df_err_removed = merged_with_notes[merged_with_notes['ISERROR'].isna()]\n",
    "filter_df_err_removed_dedup = filter_df_err_removed.drop_duplicates(subset=['SUBJECT_ID', 'TEXT'])\n",
    "print(f\"Unique number of subjects:{filter_df_err_removed_dedup['SUBJECT_ID'].nunique()}\")\n",
    "print(f\"Unique number of HADM:{filter_df_err_removed_dedup['HADM_ID'].nunique()}\")\n",
    "print(f\"Total number of notes:{filter_df_err_removed_dedup.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kzatDg0v1nKC"
   },
   "outputs": [],
   "source": [
    "df_icd9.dropna(subset= 'ICD9_CODE', inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2M8Qv1fAsqHM"
   },
   "source": [
    "### **2.6 FarSight aggregation**\n",
    "We need to consolidate all the code for a patient for multilabel prediction. In the paper, the author mentioned ICD-9 codes of a given admission from\n",
    "MIMIC-III are mapped into 19 distinct diagnostic groups, reference paper 9,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WWH3Rw4sokNw"
   },
   "outputs": [],
   "source": [
    "def group_icd9(icd_code):\n",
    "    code = str(icd_code).strip().upper()\n",
    "\n",
    "    # assign ref and V code to group 19\n",
    "    if code.startswith(('V', 'E')):\n",
    "        return 19\n",
    "\n",
    "    # transform the icd9 code, add decimal after the 3rd digit\n",
    "    if code.isdigit() and len(code) > 3:\n",
    "        real_code = code[:3] + '.' + code[3:]\n",
    "    else:\n",
    "      return 19\n",
    "\n",
    "    try:\n",
    "        num_code = float(real_code)\n",
    "    except ValueError:\n",
    "        return 19\n",
    "\n",
    "    if 1 <= num_code <= 139:\n",
    "        return 1\n",
    "    elif 140 <= num_code <= 239:\n",
    "        return 2\n",
    "    elif 240 <= num_code <= 279:\n",
    "        return 3\n",
    "    elif 280 <= num_code <= 289:\n",
    "        return 4\n",
    "    elif 290 <= num_code <= 319:\n",
    "        return 5\n",
    "    elif 320 <= num_code <= 389:\n",
    "        return 6\n",
    "    elif 390 <= num_code <= 459:\n",
    "        return 7\n",
    "    elif 460 <= num_code <= 519:\n",
    "        return 8\n",
    "    elif 520 <= num_code <= 579:\n",
    "        return 9\n",
    "    elif 580 <= num_code <= 629:\n",
    "        return 10\n",
    "    elif 630 <= num_code <= 679:\n",
    "        return 11\n",
    "    elif 680 <= num_code <= 709:\n",
    "        return 12\n",
    "    elif 710 <= num_code <= 739:\n",
    "        return 13\n",
    "    elif 740 <= num_code <= 759:\n",
    "        return 14\n",
    "    elif 780 <= num_code <= 789:\n",
    "        return 15\n",
    "    elif 790 <= num_code <= 796:\n",
    "        return 16\n",
    "    elif 797 <= num_code <= 799:\n",
    "        return 17\n",
    "    elif 800 <= num_code <= 789:\n",
    "        return 18\n",
    "\n",
    "    else:\n",
    "        return 19\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZFOQrjiYokRc"
   },
   "outputs": [],
   "source": [
    "# transform code column\n",
    "df_icd9['CODE_GROUP'] = df_icd9['ICD9_CODE'].apply(group_icd9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IO1YCekcI8wq"
   },
   "outputs": [],
   "source": [
    "# aggregate icd9 code per patients\n",
    "df_icd9_agg  = df_icd9.groupby(['SUBJECT_ID', 'HADM_ID'])['CODE_GROUP'].apply(lambda x: sorted(set(x))).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bQEjRA-2faKq",
    "outputId": "0f55e804-958e-4e4a-c0e8-8eaf1a3c3c6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique number of subjects:6520\n",
      "Unique number of HADM:6520\n",
      "Total number of notes:140681\n"
     ]
    }
   ],
   "source": [
    "# FarSight aggregation, join with the other table\n",
    "df_final_farsight = pd.merge(filter_df_err_removed_dedup, df_icd9_agg, on=[\"SUBJECT_ID\", \"HADM_ID\"], how=\"inner\")\n",
    "print(f\"Unique number of subjects:{df_final_farsight['SUBJECT_ID'].nunique()}\")\n",
    "print(f\"Unique number of HADM:{df_final_farsight['HADM_ID'].nunique()}\")\n",
    "print(f\"Total number of notes:{df_final_farsight.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CLcRwM88db-d"
   },
   "outputs": [],
   "source": [
    "df_final_farsight1 = df_final_farsight[['SUBJECT_ID', 'HADM_ID', 'TEXT', 'CODE_GROUP']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3RaDiiQudyXW"
   },
   "outputs": [],
   "source": [
    "df_final_farsight1.to_csv('df_final_farsight.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dXYQfqUrYkAC"
   },
   "source": [
    "### **2.7 Tokenization and Stopword Removal**\n",
    "First, we removed multiple spaces and special characters. Next, employed the\n",
    "NLTK tokenizer to facilitate the tokenization of nursing text. Utilizing the NLTK English stopword corpus, we removed stopwords from the generated tokens. Furthermore, punctua\u0002tion marks (except hyphens and slashes) were also removed.\n",
    "References to images (e.g., MRI_Scan.jpeg) were removed, and character case folding was performed. Before any further processing, medical con\u0002cept normalization through disambiguation of abbreviations (into their respective long forms) was facilitated using CARD, an open-source framework for clinical abbreviation recognition and disambiguation. Lastly, suffix stripping was performed through stemming, followed by lemmatization for the conversion of the stripped tokens into their respective base forms. Additionally, we eliminated\n",
    "the tokens appearing in less than ten nursing notes (e.g.,spot, cope, and inch) in order to lower the computational complexity of training (the total number of tokens pre- and post-elimination were 188,742 and 32,687 respectively)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EczLz0H1geug"
   },
   "outputs": [],
   "source": [
    "df_final_farsight = pd.read_csv('df_final_farsight.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bsRVtwrZ2du4",
    "outputId": "f9e4dd61-6d6c-4b00-ae5b-e970ae57a1c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<scispacy.abbreviation.AbbreviationDetector at 0x7fefb0de7d50>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scispacy\n",
    "from scispacy.abbreviation import AbbreviationDetector\n",
    "\n",
    "nlp = spacy.load(\"en_core_sci_sm\")\n",
    "nlp.add_pipe(\"abbreviation_detector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vuS0ledn2Tvc"
   },
   "outputs": [],
   "source": [
    "def preprocess_clinical_text_column(text_column, min_doc_freq=10):\n",
    "\n",
    "    # Load scispaCy model and abbreviation detector\n",
    "    nlp = spacy.load(\"en_core_sci_sm\")\n",
    "    nlp.add_pipe(\"abbreviation_detector\")\n",
    "\n",
    "    # Set up tools and patterns\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    punct_to_remove = ''.join(p for p in string.punctuation if p not in ['-', '/'])\n",
    "    punct_pattern = re.compile(f\"[{re.escape(punct_to_remove)}]\")\n",
    "    image_pattern = re.compile(r'\\S+\\.(jpg|jpeg|png|gif|bmp|tiff|dicom)', re.IGNORECASE)\n",
    "\n",
    "    # Tokenizer pipeline per document\n",
    "    def preprocess_text(text):\n",
    "        if not isinstance(text, str):\n",
    "            return []\n",
    "\n",
    "        text = text.lower()\n",
    "        # Remove image references\n",
    "        text = image_pattern.sub('', text)\n",
    "\n",
    "        # Clean unwanted characters (preserve hyphen/slash)\n",
    "        text = re.sub(r'[^A-Za-z0-9\\s\\-/]', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip().lower()\n",
    "\n",
    "        # Expand abbreviations using scispaCy\n",
    "        try:\n",
    "            doc = nlp(text)\n",
    "            for abrv in doc._.abbreviations:\n",
    "                text = text.replace(str(abrv), str(abrv._.long_form))\n",
    "        except:\n",
    "            pass  # continue even if abbreviation detection fails\n",
    "\n",
    "        # Tokenize with NLTK\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        # Remove stopwords and unwanted punctuation\n",
    "        tokens = [t for t in tokens if t not in stop_words and not punct_pattern.match(t)]\n",
    "\n",
    "        # Stemming + lemmatization\n",
    "        tokens = [lemmatizer.lemmatize(stemmer.stem(t)) for t in tokens]\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    # Apply to the column\n",
    "    processed_tokens = text_column.apply(preprocess_text)\n",
    "\n",
    "    # Compute document frequency\n",
    "    doc_freq = Counter()\n",
    "    for tokens in processed_tokens:\n",
    "        for token in set(tokens):\n",
    "            doc_freq[token] += 1\n",
    "\n",
    "    # Filter out rare tokens\n",
    "    filtered_tokens = processed_tokens.apply(\n",
    "        lambda tokens: [t for t in tokens if doc_freq[t] >= min_doc_freq]\n",
    "    )\n",
    "\n",
    "    return filtered_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OMcVQcXd7PTM",
    "outputId": "b1112b21-3b3f-455d-d375-9cf051de0978"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/scispacy/abbreviation.py:248: UserWarning: [W036] The component 'matcher' does not have any patterns defined.\n",
      "  global_matches = self.global_matcher(doc)\n"
     ]
    }
   ],
   "source": [
    "#remove low frequency words\n",
    "df_final_farsight['processed_tokens'] = preprocess_clinical_text_column(df_final_farsight['TEXT'], min_doc_freq=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "POjlrPwM4Bta",
    "outputId": "46cdfa8f-4173-4a6e-9ca4-a23e8483acea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct tokens: 33181\n"
     ]
    }
   ],
   "source": [
    "#32687 according to C 2) on page 7\n",
    "distinct_tokens = {token for tokens in df_final_farsight['processed_tokens'] for token in tokens}\n",
    "print(\"Number of distinct tokens:\", len(distinct_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "y2_rn3aSrSC4"
   },
   "outputs": [],
   "source": [
    "df_final_farsight.to_pickle('df_farsight_token.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YdwXdvqO_1Kf"
   },
   "source": [
    "## **3. Vectorization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "_wJ-WyPq_55P"
   },
   "outputs": [],
   "source": [
    "# read in token data\n",
    "df_farsight_token = pd.read_pickle('df_farsight_token.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "AzKp_BLYQHYu"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "def split_data(df, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Split a DataFrame into train and test sets and binarize multi-label targets.\n",
    "    Returns:\n",
    "        X_train, X_test: Lists of tokenized input sequences.\n",
    "        y_train_bin, y_test_bin: Binarized label arrays.\n",
    "\n",
    "    \"\"\"\n",
    "    X = df['processed_tokens']\n",
    "    y = df['CODE_GROUP']  # List of labels per sample (multi-label)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    y_train_bin = mlb.fit_transform(y_train)\n",
    "    y_test_bin = mlb.transform(y_test)\n",
    "\n",
    "    return X_train, X_test, y_train_bin, y_test_bin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "LAoJcAK6z5o1"
   },
   "outputs": [],
   "source": [
    "X_train_far, X_test_far, y_train_far, y_test_far = split_data(df_farsight_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Qo_NjIyUshJ"
   },
   "source": [
    "### **3.1 NMF on BoW with Semantic Coherence(SC)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ybVv3pZTWVo",
    "outputId": "167f3909-0fee-4f51-f645-bb6eb23e1a4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Coherence Score (c_v): 0.4764419067504403\n",
      "Testing Coherence Score (c_v): 0.47424322919956036\n",
      "Topic 62: ['s/p', 'left', 'fractur', 'fx', 'pain', 'need', 'pt', 'fall', 'trauma', 'injuri']\n",
      "Topic 47: ['note', 'l', 'requir', 'v', 'day', 'intub', 'chronic', 'area', 'shift', 'admiss']\n",
      "Topic 82: ['-', 'right', 'r', 'fractur', 'show', 'pt', 'l', 'side', 'hip', 'osh']\n",
      "Topic 78: ['pain', 'start', 'abdomin', 'deni', 'need', 'ed', 'nausea', 'abdomen', 'day', 'present']\n",
      "Topic 0: ['04', '/', 'meq/l', 'mg/dl', 'valuabl', 'o2', 'transfer', 'weight', 'heart', 'ml']\n",
      "Topic 91: ['remain', 'assess', 'intub', 'respons', 'plan', 'action', 'elev', 'output', 'acut', 'unchang']\n",
      "Topic 33: ['continu', 'order', 'per', 'lab', 'monitor', 'close', 'toler', 'support', 'team', 'follow']\n",
      "Topic 14: ['pt', 'time', 'also', 'assess', 'throughout', 'abl', 'bed', 'medic', 'monitor', 'state']\n",
      "Topic 51: ['pt', 'plan', 'assess', 'today', 'action', 'respons', 'pain', 'abl', 'back', 'transfer']\n",
      "Topic 31: ['assess', 'plan', 'respons', 'action', 'h/o', 'alter', 'chronic', 'impair', 'delirium', 'diseas']\n"
     ]
    }
   ],
   "source": [
    "# ====================\n",
    "# TRAINING PHASE\n",
    "# ====================\n",
    "# Build a dictionary and corpus using the training tokens.\n",
    "train_tokens = X_train_far.tolist()  # each element is a list of tokens\n",
    "dictionary_train = Dictionary(train_tokens)\n",
    "corpus_train = [dictionary_train.doc2bow(tokens) for tokens in train_tokens]\n",
    "\n",
    "# Train the NMF model on the training corpus.\n",
    "num_topics = 100  # optimal number based on your experiments\n",
    "nmf_train = Nmf(corpus=corpus_train, id2word=dictionary_train, num_topics=num_topics, random_state=42)\n",
    "\n",
    "# Compute the semantic coherence (SC) on the training set using the c_v measure (based on NPMI).\n",
    "coherence_model_train = CoherenceModel(\n",
    "    model=nmf_train,\n",
    "    texts=train_tokens,\n",
    "    dictionary=dictionary_train,\n",
    "    coherence='c_v'\n",
    ")\n",
    "coherence_train = coherence_model_train.get_coherence()\n",
    "print(\"Training Coherence Score (c_v):\", coherence_train)\n",
    "\n",
    "# ====================\n",
    "# TESTING PHASE\n",
    "# ====================\n",
    "# Prepare the test tokens using the same dictionary.\n",
    "test_tokens = X_test_far.tolist()\n",
    "corpus_test = [dictionary_train.doc2bow(tokens) for tokens in test_tokens]\n",
    "\n",
    "# Compute the semantic coherence on the test set.\n",
    "# Note: We use the same trained NMF model and dictionary.\n",
    "coherence_model_test = CoherenceModel(\n",
    "    model=nmf_train,\n",
    "    texts=test_tokens,\n",
    "    dictionary=dictionary_train,\n",
    "    coherence='c_v'\n",
    ")\n",
    "coherence_test = coherence_model_test.get_coherence()\n",
    "print(\"Testing Coherence Score (c_v):\", coherence_test)\n",
    "\n",
    "# ====================\n",
    "# OPTIONAL: Inspect Topics\n",
    "# ====================\n",
    "# Print the top 10 words for the first 10 topics for a quick inspection.\n",
    "topics = nmf_train.show_topics(num_topics=10, num_words=10, formatted=False)\n",
    "for topic_no, topic in topics:\n",
    "    topic_words = [word for word, prob in topic]\n",
    "    print(f\"Topic {topic_no}: {topic_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4S62ppDFjSne",
    "outputId": "e1f48ff9-704b-4bc7-fc8c-5632f6a07d5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training NMF vectors shape: (112544, 100)\n",
      "Testing NMF vectors shape: (28137, 100)\n",
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "def get_topic_vector(model, dictionary, tokens, num_topics):\n",
    "    \"\"\"\n",
    "    Given a trained NMF model, a dictionary, and a tokenized document,\n",
    "    returns a fixed-length topic vector representing the document's topic distribution.\n",
    "    \"\"\"\n",
    "    # Convert the document into bag-of-words format.\n",
    "    bow = dictionary.doc2bow(tokens)\n",
    "    # Get the topic distribution for the document.\n",
    "    # Using minimum_probability=0 ensures that each topic is included (if supported).\n",
    "    topic_dist = model.get_document_topics(bow, minimum_probability=0)\n",
    "    # Convert list of (topic_id, probability) to a dictionary.\n",
    "    topic_dict = dict(topic_dist)\n",
    "    # Create a fixed-length vector for all topics.\n",
    "    vector = [topic_dict.get(i, 0) for i in range(num_topics)]\n",
    "    return vector\n",
    "\n",
    "# Now compute the topic vectors for the training set.\n",
    "train_topic_vectors = [get_topic_vector(nmf_train, dictionary_train, tokens, num_topics)\n",
    "                       for tokens in train_tokens]\n",
    "\n",
    "# Similarly for the test set.\n",
    "test_topic_vectors = [get_topic_vector(nmf_train, dictionary_train, tokens, num_topics)\n",
    "                      for tokens in test_tokens]\n",
    "\n",
    "# Optionally, convert to NumPy arrays:\n",
    "import numpy as np\n",
    "train_topic_vectors = np.array(train_topic_vectors)\n",
    "test_topic_vectors = np.array(test_topic_vectors)\n",
    "\n",
    "print(\"Training NMF vectors shape:\", train_topic_vectors.shape)\n",
    "print(\"Testing NMF vectors shape:\", test_topic_vectors.shape)\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "path = \"/content/gdrive/My Drive/BDH_Project/data\"\n",
    "os.chdir(path)\n",
    "np.save('train_topic_vectors.npy', train_topic_vectors)\n",
    "np.save('test_topic_vectors.npy', test_topic_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "E9cuTwSpE0xF"
   },
   "outputs": [],
   "source": [
    "train_topic_vectors = np.load('train_topic_vectors.npy')\n",
    "test_topic_vectors = np.load('test_topic_vectors.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BURdKCKxFGWv"
   },
   "source": [
    "## **4. Predictive Modeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lgYehwtVFand"
   },
   "source": [
    "Creating Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "tH8e8gS1mTqo"
   },
   "outputs": [],
   "source": [
    "def evaluate(y_test, y_pred, y_pred_prob):\n",
    "    num_classes = y_test.shape[1]\n",
    "\n",
    "    # AUROC per label\n",
    "    auc_list = []\n",
    "    for i in range(num_classes):\n",
    "        y_true_label = y_test[:, i]\n",
    "        y_pred_label = y_pred_prob[:, i]\n",
    "        if len(np.unique(y_true_label)) < 2:\n",
    "            continue  # Skip if AUROC is undefined\n",
    "        auc = roc_auc_score(y_true_label, y_pred_label)\n",
    "        auc_list.append(auc)\n",
    "    auroc = np.mean(auc_list) if auc_list else np.nan\n",
    "\n",
    "    # AUPRC per label\n",
    "    ap_list = []\n",
    "    for i in range(num_classes):\n",
    "        y_true_label = y_test[:, i]\n",
    "        y_pred_label = y_pred_prob[:, i]\n",
    "        if len(np.unique(y_true_label)) < 2:\n",
    "            continue  # Skip if AUPRC is undefined\n",
    "        ap = average_precision_score(y_true_label, y_pred_label)\n",
    "        ap_list.append(ap)\n",
    "    auprc = np.mean(ap_list) if ap_list else np.nan\n",
    "\n",
    "    return {\n",
    "        'label_wise ACC': np.mean(y_test == y_pred),\n",
    "        'F1': f1_score(y_test, y_pred, average='samples'),\n",
    "        'MCC': matthews_corrcoef(y_test.flatten(), y_pred.flatten()),\n",
    "        'AUROC': auroc,\n",
    "        'AUPRC': auprc\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "erTQrhpvFf3w"
   },
   "source": [
    "### **4.1. Model Training - MLP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 641
    },
    "id": "6qDSFntImYCn",
    "outputId": "19ceb51a-0e07-4c17-ef6c-03adbbda7cb0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">7,575</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,064</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m75\u001b[0m)             │         \u001b[38;5;34m7,575\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m)             │         \u001b[38;5;34m1,064\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,639</span> (33.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m8,639\u001b[0m (33.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,639</span> (33.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m8,639\u001b[0m (33.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "\u001b[1m880/880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.4471 - val_loss: 0.3390\n",
      "Epoch 2/8\n",
      "\u001b[1m880/880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.3370 - val_loss: 0.3320\n",
      "Epoch 3/8\n",
      "\u001b[1m880/880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.3316 - val_loss: 0.3298\n",
      "Epoch 4/8\n",
      "\u001b[1m880/880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.3294 - val_loss: 0.3285\n",
      "Epoch 5/8\n",
      "\u001b[1m880/880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.3288 - val_loss: 0.3276\n",
      "Epoch 6/8\n",
      "\u001b[1m880/880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.3276 - val_loss: 0.3270\n",
      "Epoch 7/8\n",
      "\u001b[1m880/880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.3265 - val_loss: 0.3265\n",
      "Epoch 8/8\n",
      "\u001b[1m880/880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.3261 - val_loss: 0.3260\n",
      "\u001b[1m880/880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'label_wise ACC': 0.8347422560025183,\n",
       " 'F1': 0.8922415173692541,\n",
       " 'MCC': 0.48121518710428884,\n",
       " 'AUROC': 0.7477251432201757,\n",
       " 'AUPRC': 0.8580636182649178}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming:\n",
    "# - train_topic_vectors and test_topic_vectors are already NumPy arrays (e.g., shape (num_samples, 100))\n",
    "# - y_train_far_bin and y_test_far_bin are binarized label arrays with shape (num_samples, 19)\n",
    "\n",
    "input_dim = train_topic_vectors.shape[1]  # e.g., 100 topics\n",
    "num_classes = y_train_far.shape[1]      # 19 classes\n",
    "\n",
    "# Build the MLP model\n",
    "model = Sequential()\n",
    "model.add(Dense(75, activation='relu', input_shape=(input_dim,)))\n",
    "model.add(Dense(num_classes, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer=Adam(), loss='binary_crossentropy')\n",
    "model.summary()\n",
    "\n",
    "# Train the MLP model\n",
    "history = model.fit(\n",
    "    train_topic_vectors,\n",
    "    y_train_far,\n",
    "    batch_size=128,\n",
    "    epochs=8,\n",
    "    validation_data=(test_topic_vectors, y_test_far)\n",
    ")\n",
    "\n",
    "# Obtain predictions on the test set (probabilities)\n",
    "y_pred_prob = model.predict(test_topic_vectors)\n",
    "# Threshold probabilities to get binary predictions (using 0.5 as cutoff)\n",
    "y_pred = (y_pred_prob >= 0.5).astype(int)\n",
    "\n",
    "# Compute evaluation metrics\n",
    "evaluate(y_test_far, y_pred, y_pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H-SE82-IFbQT"
   },
   "source": [
    "### **4.2. Model Training - MLP on Pretrained Bio_ClinicalBERT**\n",
    "Please refer to the following for more details: https:\n",
    "//huggingface.co/emilyalsentzer/Bio_Clin\n",
    "icalBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "uzCJ-k1WFyof"
   },
   "outputs": [],
   "source": [
    "def split_data(df, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Split a DataFrame into train and test sets and binarize multi-label targets.\n",
    "    Returns:\n",
    "        X_train, X_test: Lists of tokenized input sequences.\n",
    "        y_train_bin, y_test_bin: Binarized label arrays.\n",
    "\n",
    "    \"\"\"\n",
    "    X = df['TEXT']\n",
    "    y = df['CODE_GROUP']  # List of labels per sample (multi-label)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    y_train_bin = mlb.fit_transform(y_train)\n",
    "    y_test_bin = mlb.transform(y_test)\n",
    "\n",
    "    return X_train, X_test, y_train_bin, y_test_bin, mlb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "5VEV8Z9IFciF"
   },
   "outputs": [],
   "source": [
    "X_train_far, X_test_far, y_train_far, y_test_far, mlb_far = split_data(df_farsight_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 949,
     "referenced_widgets": [
      "b0aadd87576546e0832946e8c574a869",
      "7218c58883d540ea94e72d95c35468e8",
      "c126a0cafe704162b561ed3485b60b12",
      "d14a502f185e4352a12ddcd94325e6f2",
      "186b7079e14a4fbe814c243c44514f07",
      "6467a4f94bc449629774fc1f58b38c8b",
      "e02f20467f314c6ea10c1975d0d3595a",
      "1f604e3800324630add3671e211b056a",
      "6f554a5f161241cb8b361e16135c8a22",
      "faa55497ad1f4b0b85278c54da588512",
      "aee395aed07d4677ab691644080f1f16",
      "4e5edde889ed4e7cb8bbfc18a98117a0",
      "c83ee82099a64d7f99e5201f8f90fd84",
      "c7fe380cca6c4c9abc2649868b945427",
      "1c97352f7ce843a1a038904bbfeb6cd3",
      "99292a7d4430487d9accbee077afd004",
      "3ed554800d964aedb5605264ba079400",
      "b1c7d6da54504b72b740d6dbb531ebee",
      "9dd447f7ca5f498d81ef6929ec106840",
      "c4ff4603b0f54fcca8c2f1b08d6fe59d",
      "f4afe4aac8df4d2aa1840ef826483a17",
      "4ab0eb5729f54596b43b50ed65baf5a1",
      "a80cf0c5cb9046aeb72b3d4fb71ef312",
      "cfd5042897d94add8e229b4b2e257a58",
      "478cabcd00aa46489ddf96d11fae75ad",
      "559b468473d64f5db158f68dc711a405",
      "9db45d03626d4b4e8e52c495c0d8e54d",
      "90bc718c2adc4991bd7ddcc94b1ac863",
      "f7682d7f86cd45958efb6b892e9aa843",
      "e64dda0a897842e7991fd08be0c41b54",
      "2b3db4fb7853489895d0357ff388e94d",
      "feced8dc5581427cbe90f6e1c1b93918",
      "83cba887638b412e8b68a3c5d7ff6c57"
     ]
    },
    "id": "sDiXbSvlFqTn",
    "outputId": "f79812b8-fa04-4d7c-e6b9-98e8a1d67fe1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0aadd87576546e0832946e8c574a869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e5edde889ed4e7cb8bbfc18a98117a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a80cf0c5cb9046aeb72b3d4fb71ef312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "kDPFCKuQF9Y-"
   },
   "outputs": [],
   "source": [
    "def get_bert_embedding(text, tokenizer, model, device):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Return [CLS] token (first token)\n",
    "    return outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7NB7fsnfF_sH",
    "outputId": "9ff6d5b7-bf74-4672-9edb-92c435f41c29"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding notes: 100%|██████████| 112544/112544 [50:04<00:00, 37.46it/s]\n",
      "Embedding notes: 100%|██████████| 28137/28137 [12:32<00:00, 37.41it/s]\n"
     ]
    }
   ],
   "source": [
    "def embed_texts(texts, tokenizer, model, device):\n",
    "    embeddings = []\n",
    "    for text in tqdm(texts, desc=\"Embedding notes\"):\n",
    "        try:\n",
    "            emb = get_bert_embedding(text, tokenizer, model, device)\n",
    "        except Exception as e:\n",
    "            print(\"Skipping text due to error:\", e)\n",
    "            emb = np.zeros(768)  # fallback\n",
    "        embeddings.append(emb)\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "X_train_Far = embed_texts(X_train_far, tokenizer, model, device)\n",
    "X_test_Far = embed_texts(X_test_far, tokenizer, model, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "STqRusVcPPgP",
    "outputId": "e2872ee0-828c-4ba5-e25b-d5bbfd66a97b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "\u001b[1m880/880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.0030 - loss: 0.3616 - val_accuracy: 0.0000e+00 - val_loss: 0.3238\n",
      "Epoch 2/8\n",
      "\u001b[1m880/880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.3235 - val_accuracy: 0.0000e+00 - val_loss: 0.3211\n",
      "Epoch 3/8\n",
      "\u001b[1m880/880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 3.3227e-05 - loss: 0.3196 - val_accuracy: 0.0000e+00 - val_loss: 0.3194\n",
      "Epoch 4/8\n",
      "\u001b[1m880/880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 7.3258e-05 - loss: 0.3176 - val_accuracy: 3.5540e-05 - val_loss: 0.3173\n",
      "Epoch 5/8\n",
      "\u001b[1m880/880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 8.7737e-05 - loss: 0.3155 - val_accuracy: 3.5540e-05 - val_loss: 0.3149\n",
      "Epoch 6/8\n",
      "\u001b[1m880/880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 8.0138e-05 - loss: 0.3136 - val_accuracy: 3.5540e-05 - val_loss: 0.3141\n",
      "Epoch 7/8\n",
      "\u001b[1m880/880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 9.4847e-05 - loss: 0.3132 - val_accuracy: 3.5540e-05 - val_loss: 0.3137\n",
      "Epoch 8/8\n",
      "\u001b[1m880/880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 3.5172e-05 - loss: 0.3110 - val_accuracy: 0.0000e+00 - val_loss: 0.3119\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "mlp = Sequential([\n",
    "    Dense(75, activation='relu', input_shape=(768,)),\n",
    "    Dense(len(mlb_far.classes_), activation='sigmoid')\n",
    "])\n",
    "mlp.compile(optimizer=Adam(learning_rate=0.001),\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "\n",
    "history = mlp.fit(X_train_Far, y_train_far,\n",
    "                  validation_data=(X_test_Far, y_test_far),\n",
    "                  epochs=8,\n",
    "                  batch_size=128,\n",
    "                  callbacks=[early_stop],\n",
    "                  verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kAr3cKw5PV9V",
    "outputId": "ed7107a4-aff2-47c6-82ac-595f936d4f88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m880/880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'label_wise ACC': 0.8440360684203312,\n",
       " 'F1': 0.898134689075536,\n",
       " 'MCC': 0.5100901223100083,\n",
       " 'AUROC': 0.7872540939812089,\n",
       " 'AUPRC': 0.8817496412484611}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_prob = mlp.predict(X_test_Far)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "evaluate(y_test_far, y_pred, y_pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zV1jSEFWLhbb"
   },
   "source": [
    "### **4.3. Model Training - Finetuned Bio_ClinicalBERT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Muu7EALUPvLt"
   },
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "df_farsight_token['labels'] = mlb.fit_transform(df_farsight_token['CODE_GROUP']).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "Jxo10c1IPy8k"
   },
   "outputs": [],
   "source": [
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df_farsight_token['TEXT'].tolist(), df_farsight_token['labels'].tolist(), test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "7uz-zWQGP1Vi"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "b18216bad3df4e55af6f4fe7f7aa5f62",
      "625b9ff94e784fe3a1e45bc377737511",
      "4eb0e1910147488fa949cafc253ead2a",
      "7c041a9ad0b84c4b9e2306da10dbb233",
      "3bc36499d853496bb78feacf95f1d6e8",
      "ca8b989d1a1840a696bb73c690e149b3",
      "48d9d4bfeb244e4a8aed5fb8436a2fa6",
      "9d8b62fbf32945f79e34319a2e15748b",
      "f65e4e8a1a76466aa042f3701b5ebcc7",
      "bfaf8933ee4c4294bc454a050786731a",
      "3389150ab7a64b41b2d89265690d0e54",
      "e1f6ebe123e94a9588dc7fa75d684c10",
      "5dcd7688734649aaa14a985158182822",
      "a88b1a8c7ae546219ce36728c2fde9f6",
      "5f7244324ccf4406ace4fcdb3155ff68",
      "074e9e2019804a4891a026e03e279fa4",
      "81acad441b6044f8b703c6a54b251616",
      "bfa4ddd402bc4738a95bb8e4719c85c6",
      "1a8bd462fabe452c9ae8cb1b6f50f76f",
      "1e403d48d47b479592735d4e16d5afd8",
      "3ad0862491f44a2faa5793fe8f5c2695",
      "a6a7fbc82be34acc8c2a1bbf91e2425a"
     ]
    },
    "id": "XLfZNONZP5n3",
    "outputId": "ed4db68e-b188-4021-f8f2-d50ed4b63cf6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b18216bad3df4e55af6f4fe7f7aa5f62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/112544 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1f6ebe123e94a9588dc7fa75d684c10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/28137 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_ds = Dataset.from_dict({\"text\": train_texts, \"labels\": train_labels})\n",
    "test_ds = Dataset.from_dict({\"text\": test_texts, \"labels\": test_labels})\n",
    "\n",
    "\n",
    "train_ds = Dataset.from_dict({\n",
    "    \"text\": train_texts,\n",
    "    \"labels\": [np.array(lbl, dtype=np.float32) for lbl in train_labels]\n",
    "})\n",
    "\n",
    "test_ds = Dataset.from_dict({\n",
    "    \"text\": test_texts,\n",
    "    \"labels\": [np.array(lbl, dtype=np.float32) for lbl in test_labels]\n",
    "})\n",
    "\n",
    "\n",
    "train_ds = train_ds.map(tokenize, batched=True)\n",
    "test_ds = test_ds.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yYB3N7LkP77N",
    "outputId": "5da3e7f7-7224-4a6f-aa42-47b46ef91830"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"emilyalsentzer/Bio_ClinicalBERT\",\n",
    "    num_labels=len(train_labels[0]),\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "4klTuwN_QUT2"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"clinicalbert-multilabel\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=4,\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 516
    },
    "id": "NAinLaZ0QXSI",
    "outputId": "47279016-2c0b-409a-e549-8f9496c2569e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
       "            function loadScript(url) {\n",
       "            return new Promise(function(resolve, reject) {\n",
       "                let newScript = document.createElement(\"script\");\n",
       "                newScript.onerror = reject;\n",
       "                newScript.onload = resolve;\n",
       "                document.body.appendChild(newScript);\n",
       "                newScript.src = url;\n",
       "            });\n",
       "            }\n",
       "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
       "            const iframe = document.createElement('iframe')\n",
       "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
       "            document.body.appendChild(iframe)\n",
       "            const handshake = new Postmate({\n",
       "                container: iframe,\n",
       "                url: 'https://wandb.ai/authorize'\n",
       "            });\n",
       "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
       "            handshake.then(function(child) {\n",
       "                child.on('authorize', data => {\n",
       "                    clearTimeout(timeout)\n",
       "                    resolve(data)\n",
       "                });\n",
       "            });\n",
       "            })\n",
       "        });\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "wandb: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ··········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmr-yuanzhen\u001b[0m (\u001b[33mmr-yuanzhen-georgia-tech-alumni-association\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/gdrive/MyDrive/BDH_Project/data/wandb/run-20250517_025939-lv2qpwvz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mr-yuanzhen-georgia-tech-alumni-association/huggingface/runs/lv2qpwvz' target=\"_blank\">clinicalbert-multilabel</a></strong> to <a href='https://wandb.ai/mr-yuanzhen-georgia-tech-alumni-association/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mr-yuanzhen-georgia-tech-alumni-association/huggingface' target=\"_blank\">https://wandb.ai/mr-yuanzhen-georgia-tech-alumni-association/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mr-yuanzhen-georgia-tech-alumni-association/huggingface/runs/lv2qpwvz' target=\"_blank\">https://wandb.ai/mr-yuanzhen-georgia-tech-alumni-association/huggingface/runs/lv2qpwvz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='56272' max='56272' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [56272/56272 13:29:36, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.228300</td>\n",
       "      <td>0.209806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.141000</td>\n",
       "      <td>0.150798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.115500</td>\n",
       "      <td>0.121048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.079400</td>\n",
       "      <td>0.115400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=56272, training_loss=0.1619539380493, metrics={'train_runtime': 48608.8941, 'train_samples_per_second': 9.261, 'train_steps_per_second': 1.158, 'total_flos': 1.1845904423426458e+17, 'train_loss': 0.1619539380493, 'epoch': 4.0})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "id": "WvSDDBTZQaWZ",
    "outputId": "e56f5ed3-8d72-4057-d5b9-7cb7ba5fdfbf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'label_wise ACC': 0.9544778354886042,\n",
       " 'F1': 0.9688195782561334,\n",
       " 'MCC': 0.865253719468421,\n",
       " 'AUROC': 0.9726510624735064,\n",
       " 'AUPRC': 0.9878320637327382}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict\n",
    "predictions = trainer.predict(test_ds).predictions\n",
    "probs = torch.sigmoid(torch.tensor(predictions)).numpy()  # convert logits to probabilities\n",
    "\n",
    "# Evaluate\n",
    "y_true = np.array(test_labels)\n",
    "y_pred = (probs > 0.5).astype(int)  # thresholding for multilabel\n",
    "\n",
    "evaluate(y_true, y_pred, probs)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
